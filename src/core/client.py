"""
Local LLM Client Infrastructure.

This module defines the `LocalLLMClient`, a custom adapter that bridges AdalFlow
with Hugging Face Transformers. It is specifically optimized for running
experiments on consumer hardware (e.g., Google Colab T4 GPUs) by leveraging
4-bit quantization and efficient model loading.

Key responsibilities:
1. Model Loading: Handles BitsAndBytes config for low-memory usage.
2. Chat Templating: Applies the correct system/user format for Instruct models.
3. Protocol Adaptation: Converts AdalFlow inputs/outputs to Transformers format.
"""

import logging
import torch
import re
from typing import Any, Dict, Optional, List

# Third-party libraries
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# AdalFlow core imports
from adalflow.core.model_client import ModelClient
from adalflow.core.types import GeneratorOutput, ModelType

# Configure logger for this module
log = logging.getLogger(__name__)

class LocalLLMClient(ModelClient):
    """
    A custom AdalFlow client wrapper for Hugging Face Transformers models.
    
    This client is designed to run large language models locally with optimizations 
    suitable for limited hardware resources (e.g., Google Colab free tier).

    Key Features:
    - **4-bit Quantization (NF4):** Reduces VRAM usage via BitsAndBytes.
    - **Strict Chat Templating:** Ensures compatibility with Instruct models (e.g., Qwen, Llama).
    - **Robust Output Parsing:** Handles raw string outputs and cleans XML tags often injected 
      by prompt optimizers.
    """

    def __init__(self, model_name: str):
        """
        Initialize the client with a specific model identifier.

        Args:
            model_name (str): The Hugging Face model ID (e.g., 'Qwen/Qwen2.5-1.5B-Instruct').
        """
        super().__init__()
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        
        # Initialize the model immediately upon instantiation
        self._initialize_model()

    def _initialize_model(self):
        """
        Loads the model and tokenizer from Hugging Face with 4-bit quantization configuration.
        """
        log.info(f"Loading {self.model_name} with BitsAndBytes NF4 config...")
        
        # Configure 4-bit quantization to fit models into consumer GPU memory (T4/L4)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                quantization_config=bnb_config,
                device_map="auto", # Automatically distributes model across available GPUs/CPU
                trust_remote_code=True
            )
        except Exception as e:
            log.error(f"Failed to load model {self.model_name}: {e}")
            raise e

    def convert_inputs_to_api_kwargs(
        self,
        input: Any,
        model_kwargs: Dict = {},
        model_type: ModelType = ModelType.UNDEFINED,
    ) -> Dict:
        """
        Adapts AdalFlow's generic input structure to the specific arguments 
        expected by this client's `call` method.

        Args:
            input (Any): The input data (usually the rendered prompt string).
            model_kwargs (Dict): Additional generation parameters.

        Returns:
            Dict: A dictionary ready to be unpacked into `call()`.
        """
        final_args = model_kwargs.copy()
        # Map the generic 'input' to 'input_str' for clarity in the call method
        final_args["input_str"] = input
        return final_args

    def parse_chat_completion(self, completion: Any) -> GeneratorOutput:
        """
        Parses the raw response from the `call` method into a structured GeneratorOutput.
        
        This method includes logic to clean specific XML tags (like <proposed_variable>)
        that might be generated by the Text-Grad optimizer, ensuring the output 
        contains only the clean prompt content.

        Args:
            completion (Any): The raw string returned by `call()`.

        Returns:
            GeneratorOutput: The wrapped output object expected by AdalFlow components.
        """
        try:
            response_text = str(completion)

            # Clean XML Tags if present.
            # The TGD Optimizer sometimes wraps the proposed prompt in XML tags.
            # We extract the content inside <proposed_variable>...</proposed_variable>.
            if "<proposed_variable>" in response_text:
                match = re.search(r"<proposed_variable>(.*?)</proposed_variable>", response_text, re.DOTALL)
                if match:
                    response_text = match.group(1).strip()

            return GeneratorOutput(data=response_text, raw_response=str(completion))
        except Exception as e:
            # Return an error object instead of crashing if parsing fails
            return GeneratorOutput(data=None, error=str(e))

    def call(self, api_kwargs: Dict = {}, model_type: ModelType = ModelType.UNDEFINED) -> str:
        """
        Executes the forward pass (inference) using the Hugging Face model.

        This method handles:
        1. Constructing the message history (System + User).
        2. Applying the model's specific chat template.
        3. Generating the response tokens.
        4. Decoding the tokens back to string.

        Args:
            api_kwargs (Dict): Must contain 'input_str' or 'messages', plus optional 'model_kwargs'.

        Returns:
            str: The raw generated text string.
        """
        # Extract parameters
        system_prompt = api_kwargs.get("system_prompt", None)
        user_input = api_kwargs.get("input_str", "")
        gen_kwargs = api_kwargs.get("model_kwargs", {})

        messages = []
        
        # Priority 1: Use direct messages list if provided (e.g., from Optimizer logic)
        if "messages" in api_kwargs:
            messages = api_kwargs["messages"]
        # Priority 2: Construct messages from system/user prompts
        else:
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            if user_input:
                messages.append({"role": "user", "content": user_input})

        if not messages:
            log.warning("No messages provided to LocalLLMClient. Returning empty string.")
            return "" 

        try:
            # Apply Chat Template (Crucial for Instruct Models)
            text_input = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Tokenize and move to device
            model_inputs = self.tokenizer([text_input], return_tensors="pt").to(self.model.device)

            # Set generation parameters (defaults if not provided)
            max_new_tokens = gen_kwargs.get("max_new_tokens", 512)
            temperature = gen_kwargs.get("temperature", 0.7)

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=(temperature > 0),
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # Decode Response (skipping the input prompt tokens to return only the answer)
            input_length = model_inputs.input_ids.shape[1]
            generated_ids = [output_ids[input_length:] for output_ids in generated_ids]
            response_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

            return response_text # Return raw string!

        except Exception as e:
            log.error(f"Generation failed: {e}")
            return ""