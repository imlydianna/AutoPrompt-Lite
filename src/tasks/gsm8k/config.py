"""
Configuration file for the GSM8K Task.

This file acts as the single source of truth for:
1. Model Identifiers (Student/Teacher)
2. Generation Hyperparameters (Temperature, Tokens)
3. Dataset Split Sizes (Train, Val, Test)
4. Optimization/Training Loop Parameters

Usage:
    from src.tasks.gsm8k.config import STUDENT_MODEL_NAME, TRAIN_SIZE, ...
"""

# -----------------------------------------------------------------------------
# GLOBAL SETTINGS
# -----------------------------------------------------------------------------
# Setting a seed ensures reproducibility for dataset shuffling and splitting.
SEED = 42

# -----------------------------------------------------------------------------
# DATASET CONFIGURATION
# -----------------------------------------------------------------------------
# We define three distinct splits to ensure rigorous evaluation:
# 1. TRAIN: Used by the Optimizer to generate gradients and propose prompts.
# 2. VAL:   Used internally by the Trainer to validate proposals (Early Stopping/Selection).
# 3. TEST:  Used strictly for final evaluation (Baseline vs. Optimized).

# NOTE: Keep these numbers small for Google Colab Free Tier (T4 GPU).
# Increase them if running on stronger hardware (e.g., A100).
TRAIN_SIZE = 10   # Number of samples for the optimization loop
VAL_SIZE = 5      # Number of samples for validating new prompts during training
TEST_SIZE = 10    # Number of samples for the final 'evaluate.py' report

# -----------------------------------------------------------------------------
# MODEL CONFIGURATION
# -----------------------------------------------------------------------------
# Student: The model attempting to solve the math problems.
# We use a lightweight 1.5B model for faster iteration and low VRAM usage.
STUDENT_MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

# Teacher: The "Backward Engine" and "Optimizer".
# We use a stronger 7B model to provide high-quality feedback and prompt edits.
TEACHER_MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

# -----------------------------------------------------------------------------
# GENERATION PARAMETERS
# -----------------------------------------------------------------------------
# Student Parameters:
# - Temperature 0.5: Allows for some creativity in Chain-of-Thought reasoning.
# - max_new_tokens 400: Enough space for step-by-step logic.
STUDENT_MODEL_KWARGS = {
    "temperature": 0.5,
    "max_new_tokens": 400,
}

# Teacher Parameters:
# - Temperature 0.7: Slightly higher to allow diversity in proposing NEW prompts.
# - max_new_tokens 512: Needs space to explain the error (gradient) and rewrite the prompt.
TEACHER_MODEL_KWARGS = {
    "temperature": 0.7,
    "max_new_tokens": 512,
}

# -----------------------------------------------------------------------------
# TRAINING / OPTIMIZATION HYPERPARAMETERS
# -----------------------------------------------------------------------------
# Max Steps: How many optimization iterations (generations) to run.
MAX_STEPS = 2  

# Train Batch Size: 
# Set to 1 for "Stochastic" updates (update prompt after every sample).
# Helps avoid OOM on Colab and keeps the context window for the Teacher manageable.
TRAIN_BATCH_SIZE = 1

# Validation Batch Size:
# Number of inference requests to run during validation.
# Keep it low to minimize VRAM usage on T4 GPUs.
VAL_BATCH_SIZE = 1